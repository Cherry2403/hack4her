defaults:
  - rl_gsm8k
  - override finetune: rl_llama31_70b_lora
  - _self_

model_path: meta-llama/Meta-Llama-3.1-70B-Instruct

vllm_config:
  vllm_kwargs:
    --download-dir: /mnt/llmd/base_models/ 
    --gpu-memory-utilization: 0.9
    # VLLM get log probs OOM https://github.com/vllm-project/vllm/issues/5907
    --enable-chunked-prefill: ""
    --quantization: fp8