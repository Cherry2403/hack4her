diff --git a/examples/rl_gsm8k/collect_data.py b/examples/rl_gsm8k/collect_data.py
index aaaad7b..4631eee 100644
--- a/examples/rl_gsm8k/collect_data.py
+++ b/examples/rl_gsm8k/collect_data.py
@@ -282,6 +282,7 @@ def main(cfg: DictConfig):
                     print("COMPLETION", trace.output_text)
 
                     trace.fork_id = i
+                    #FIXME: this is the last reward experienced
                     trace.rewards = [reward]
                     training_samples.append(trace)
         except Exception as e:
diff --git a/examples/rl_gsm8k/math_agent.py b/examples/rl_gsm8k/math_agent.py
index 66815d0..dee7293 100644
--- a/examples/rl_gsm8k/math_agent.py
+++ b/examples/rl_gsm8k/math_agent.py
@@ -151,7 +151,6 @@ class MathAgent(MonoAgent):
         """
         We only train on the last completion
         """
-        #TODO: Oleh discussion
         _, llm_calls = self.reuse(tape)
         return [self.make_training_text(llm_call, compute_log_probs=True) for llm_call in llm_calls]
 
diff --git a/tapeagents/finetune/rl/grpo.py b/tapeagents/finetune/rl/grpo.py
index 4f45315..7fc1224 100644
--- a/tapeagents/finetune/rl/grpo.py
+++ b/tapeagents/finetune/rl/grpo.py
@@ -156,6 +156,7 @@ def grpo_step(model, batch, config: GRPOConfig) -> tuple[torch.Tensor, dict[str,
     # assert torch.isfinite(surrogate_loss).all(), "surrogate_loss contains NaN or inf"
     # assert torch.isfinite(approx_kl).all(), "approx_kl contains NaN or inf"
     loss = -masked_mean(surrogate_loss - config.kl_coef * approx_kl, masks_)
+    breakpoint()
     assert torch.isfinite(loss).all(), "loss contains NaN or inf"
     # if not torch.isfinite(loss):
     #    loss = torch.nan_to_num(loss, nan=0.0, posinf=0.0, neginf=0.0)
@@ -311,9 +312,7 @@ def prepare_rl_fields(
         old_logprobs
     ), f"Target tokens: {len(target_tokens)}, old logprobs: {len(old_logprobs)}"
 
-    encoding["rewards"] = [0.0] * (len(encoding["labels"]) - len(old_logprobs)) + rewards_per_line[:1] * len(
-        old_logprobs
-    )
+    encoding["rewards"] = rewards_per_line[:1] * len(encoding["labels"])
     encoding["advantages"] = [0.0] * len(encoding["labels"])  # place holder
     encoding["old_logprobs"] = [0.0] * (len(encoding["labels"]) - len(old_logprobs)) + old_logprobs
     encoding["ref_logprobs"] = [0.0] * (len(encoding["labels"]) - len(old_logprobs)) + ref_logprobs
