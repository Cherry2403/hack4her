import datetime
from typing import Generator, TypeAlias

import litellm
from pydantic import BaseModel, Field

from tapeagents.core import Prompt, TokenLogprob

LLMOutput: TypeAlias = litellm.utils.Message
"""Type alias for the output of the language model."""


class LLMCall(BaseModel):
    """
    LLMCall stores info about a call to a language model.

    Attributes:
        timestamp (str): The timestamp when the call was made, in ISO 8601 format.
        prompt (Prompt): The input prompt provided to the language model.
        output (LLMOutput): The output generated by the language model.
        prompt_length_tokens (int): The length of the prompt in tokens. Defaults to -1 if not set.
        output_length_tokens (int): The length of the output in tokens. Defaults to -1 if not set.
        cached (bool): Indicates whether the result was retrieved from cache.
    """

    timestamp: str = Field(default_factory=lambda: datetime.datetime.now().isoformat())
    prompt: Prompt
    output: LLMOutput
    prompt_length_tokens: int = -1
    output_length_tokens: int = -1
    cached: bool
    llm_info: dict = {}
    cost: float = 0
    logprobs: list[TokenLogprob] = Field(default_factory=list, exclude=True)


class LLMEvent(BaseModel):
    """An event class representing either a chunk of LLM output or the final LLM output.

    This class encapsulates events that occur during LLM processing, handling both
    intermediate chunks of output and the final complete output.

    Attributes:
        chunk (str, optional): A partial text output from the LLM stream.
        output (LLMOutput, optional): The complete output from the LLM.
        llm_call (LLMCall, optional): The entire LLMCall object.
    """

    chunk: str | None = None
    output: LLMOutput | None = None
    llm_call: LLMCall | None = None


class LLMStream:
    """A wrapper class for LLM generators that provides convenient iteration and output extraction.

    This class wraps a generator that yields LLMEvents and provides methods to:

    - Iterate through events
    - Extract complete LLM output
    - Get the assistant's response text

    LLMStream stores the LLM call object when the generator yields it.

    Attributes:
        generator: Generator yielding LLMEvents or None if empty
        prompt: The prompt used to generate the LLM response:

    Raises:
        ValueError: When trying to iterate null stream, when no output is produced,
                   or when output is not an assistant message with content
    """

    def __init__(self, generator: Generator[LLMEvent, None, None] | None, prompt: Prompt):
        self.generator = generator
        self.prompt = prompt

    def __bool__(self):
        return self.generator is not None

    def __iter__(self):
        if self.generator is None:
            raise ValueError("can't iterate a null stream")
        return self

    def __next__(self) -> LLMEvent:
        if self.generator is None:
            raise StopIteration
        event = next(self.generator)
        if event.llm_call:
            self.llm_call = event.llm_call
        return event

    def get_output(self) -> LLMOutput:
        """Returns first LLMOutput found in events"""
        for event in self:
            if event.output:
                return event.output
        raise ValueError("LLM did not produce an output")

    def get_text(self) -> str:
        """Returns content of first assistant message found"""
        o = self.get_output()
        if not o.role == "assistant" or o.content is None:
            raise ValueError("LLM did not produce an assistant message")
        return o.content

    def get_llm_call(self) -> LLMCall:
        """Returns the LLMCall object"""
        for event in self:
            if event.llm_call:
                break
        return self.llm_call
